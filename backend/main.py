# backend/main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import openai
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import logging
import heapq
from datetime import datetime

# ë£¨íŠ¸ í´ë”ì˜ .env íŒŒì¼ ë¡œë“œ
root_dir = Path(__file__).parent.parent
env_path = root_dir / '.env'
load_dotenv(env_path)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(title="Korean Word Graph API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
openai_client = None
word_vectors = {}
word_cache = {}

# ìš°ì„ ìˆœìœ„ í (í™í)
word_queue = []  # Min-heap: (timestamp, student_id, word, submission)
processed_words = set()  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€

# í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
def load_korean_model():
    global model
    try:
        # KoSentenceBERT ëª¨ë¸ ë¡œë“œ (í•œêµ­ì–´ íŠ¹í™”)
        model_name = "jhgan/ko-sroberta-multitask"
        model = SentenceTransformer(model_name)
        logger.info(f"Korean embedding model loaded: {model_name}")
    except Exception as e:
        logger.error(f"Failed to load Korean model: {e}")
        # ë°±ì—… ëª¨ë¸
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        logger.info("Fallback to multilingual model")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
def initialize_openai():
    global openai_client
    api_key = os.getenv('OPENAI_API_KEY')
    if api_key:
        openai_client = openai.OpenAI(api_key=api_key)
        logger.info("OpenAI client initialized")
    else:
        logger.warning("OpenAI API key not found")

# ë°ì´í„° ëª¨ë¸
class WordInput(BaseModel):
    word: str

class SimilarityRequest(BaseModel):
    word1: str
    word2: str

class ClusterRequest(BaseModel):
    words: List[str]
    num_clusters: Optional[int] = None

class LabelRequest(BaseModel):
    words: List[str]

class HybridSimilarityResponse(BaseModel):
    vector_similarity: float
    llm_similarity: float
    hybrid_score: float
    confidence: str

class ClusterResponse(BaseModel):
    cluster_assignments: List[int]
    cluster_centers: List[List[float]]
    silhouette_score: float

# ì•± ì‹œì‘ì‹œ ëª¨ë¸ ë¡œë“œ
@app.on_event("startup")
async def startup_event():
    load_korean_model()
    initialize_openai()

# í•™ìƒ ë‹¨ì–´ ì œì¶œ ì ‘ìˆ˜
@app.post("/api/submit-word")
async def submit_word_to_queue(request: dict):
    #í•™ìƒì´ ì œì¶œí•œ ë‹¨ì–´ë¥¼ ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€
    try:
        word = request.get("word", "").strip()
        student_id = request.get("studentId", "anonymous")
        timestamp = request.get("timestamp", int(datetime.now().timestamp() * 1000))
        
        if not word:
            raise HTTPException(status_code=400, detail="ë‹¨ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€ (timestampê°€ ì‘ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        submission = {
            "word": word,
            "student_id": student_id,
            "timestamp": timestamp,
            "received_at": datetime.now().isoformat()
        }
        
        # í™íì— ì¶”ê°€ (timestamp ê¸°ì¤€ ì •ë ¬)
        heapq.heappush(word_queue, (timestamp, student_id, word, submission))
        
        logger.info(f"Word queued: {word} from {student_id} at {timestamp}")
        
        return {
            "status": "queued",
            "word": word,
            "student_id": student_id,
            "queue_position": len(word_queue),
            "timestamp": timestamp
        }
    
    except Exception as e:
        logger.error(f"Submit word error: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¨ì–´ ì œì¶œ ì‹¤íŒ¨: {str(e)}")

# íì—ì„œ ë‹¤ìŒ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°
@app.get("/api/next-word")
async def get_next_word():
    """ìš°ì„ ìˆœìœ„ íì—ì„œ ë‹¤ìŒ ì²˜ë¦¬í•  ë‹¨ì–´ ë°˜í™˜"""
    try:
        if not word_queue:
            return {
                "status": "empty",
                "queue_size": 0,
                "message": "ì²˜ë¦¬í•  ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # ê°€ì¥ ì˜¤ë˜ëœ ë‹¨ì–´ ì¶”ì¶œ
        timestamp, student_id, word, submission = heapq.heappop(word_queue)
        
        # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€
        word_key = f"{word}-{student_id}"
        if word_key in processed_words:
            # ì¤‘ë³µì´ë©´ ë‹¤ìŒ ë‹¨ì–´ë¡œ
            if word_queue:
                return await get_next_word()
            else:
                return {"status": "empty", "queue_size": 0}
        
        processed_words.add(word_key)
        
        return {
            "status": "word_ready",
            "word": word,
            "student_id": student_id,
            "timestamp": timestamp,
            "submission": submission,
            "queue_size": len(word_queue)
        }
    
    except Exception as e:
        logger.error(f"Get next word error: {e}")
        raise HTTPException(status_code=500, detail=f"ë‹¤ìŒ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}")

# í ìƒíƒœ í™•ì¸
@app.get("/api/queue-status")
async def get_queue_status():
    """í˜„ì¬ í ìƒíƒœ ë°˜í™˜"""
    try:
        # íì—ì„œ ë¯¸ë¦¬ë³´ê¸° (ì‹¤ì œë¡œ ì œê±°í•˜ì§€ ì•ŠìŒ)
        preview = []
        temp_queue = word_queue.copy()
        
        for i in range(min(5, len(temp_queue))):
            if temp_queue:
                timestamp, student_id, word, submission = temp_queue[i]
                preview.append({
                    "word": word,
                    "student_id": student_id,
                    "timestamp": timestamp,
                    "wait_time": int((datetime.now().timestamp() * 1000 - timestamp) / 1000)
                })
        
        return {
            "queue_size": len(word_queue),
            "preview": preview,
            "processed_count": len(processed_words)
        }
    
    except Exception as e:
        logger.error(f"Queue status error: {e}")
        raise HTTPException(status_code=500, detail=f"í ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

# ë¹ ë¥¸ LLM ìœ ì‚¬ë„ ê³„ì‚° (ìµœì í™”)
@app.post("/api/quick-similarity")
async def quick_llm_similarity(request: dict):
    """ë¹ ë¥¸ LLM ìœ ì‚¬ë„ ê³„ì‚° - ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì†ë„ ìµœì í™”"""
    try:
        if not openai_client:
            raise HTTPException(status_code=503, detail="OpenAI API ì‚¬ìš© ë¶ˆê°€")
        
        word1 = request.get("word1", "")
        word2 = request.get("word2", "")
        
        # ë§¤ìš° ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë¹ ë¥¸ ì‘ë‹µ
        prompt = f'"{word1}"ì™€ "{word2}" ì—°ê´€ì„± ì ìˆ˜ (0-100): '
        
        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=5,  # í† í° ìˆ˜ ìµœì†Œí™”
            temperature=0.1  # ì¼ê´€ì„± ìˆëŠ” ë‹µë³€
        )
        
        return {
            "word1": word1,
            "word2": word2,
            "response": response.choices[0].message.content.strip(),
            "method": "quick_llm"
        }
    
    except Exception as e:
        logger.error(f"Quick LLM similarity error: {e}")
        raise HTTPException(status_code=500, detail=f"ë¹ ë¥¸ LLM ìœ ì‚¬ë„ ì‹¤íŒ¨: {str(e)}")

# í•œêµ­ì–´ ë‹¨ì–´ ë²¡í„°í™”
@app.post("/api/vectorize")
async def vectorize_word(word_input: WordInput):
    """í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
    try:
        word = word_input.word.strip()
        
        # ìºì‹œ í™•ì¸
        if word in word_vectors:
            return {
                "word": word,
                "vector": word_vectors[word].tolist(),
                "cached": True
            }
        
        # ë²¡í„°í™”
        vector = model.encode([word])[0]
        word_vectors[word] = vector
        
        return {
            "word": word,
            "vector": vector.tolist(),
            "cached": False,
            "vector_dim": len(vector)
        }
    
    except Exception as e:
        logger.error(f"Vectorization error: {e}")
        raise HTTPException(status_code=500, detail=f"ë²¡í„°í™” ì‹¤íŒ¨: {str(e)}")

# ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°
@app.post("/api/vector-similarity")
async def calculate_vector_similarity(request: SimilarityRequest):
    """ë‘ ë‹¨ì–´ì˜ ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°"""
    try:
        # ë‘ ë‹¨ì–´ ë²¡í„°í™”
        word1_vector = model.encode([request.word1])[0]
        word2_vector = model.encode([request.word2])[0]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity([word1_vector], [word2_vector])[0][0]
        
        return {
            "word1": request.word1,
            "word2": request.word2,
            "similarity": float(similarity),
            "method": "cosine_similarity"
        }
    
    except Exception as e:
        logger.error(f"Vector similarity error: {e}")
        raise HTTPException(status_code=500, detail=f"ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")

# K-means í´ëŸ¬ìŠ¤í„°ë§
@app.post("/api/clustering", response_model=ClusterResponse)
async def perform_clustering(request: ClusterRequest):
    """í•œêµ­ì–´ ë‹¨ì–´ë“¤ì˜ K-means í´ëŸ¬ìŠ¤í„°ë§"""
    try:
        if len(request.words) < 2:
            raise HTTPException(status_code=400, detail="ìµœì†Œ 2ê°œ ì´ìƒì˜ ë‹¨ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ë‹¨ì–´ë“¤ ë²¡í„°í™”
        vectors = model.encode(request.words)
        
        # í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
        if request.num_clusters is None:
            num_clusters = min(max(2, len(request.words) // 3), 8)
        else:
            num_clusters = min(request.num_clusters, len(request.words))
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        cluster_assignments = kmeans.fit_predict(vectors).tolist()
        
        # ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
        from sklearn.metrics import silhouette_score
        if len(set(cluster_assignments)) > 1:
            sil_score = float(silhouette_score(vectors, cluster_assignments))
        else:
            sil_score = 0.0
        
        return ClusterResponse(
            cluster_assignments=cluster_assignments,
            cluster_centers=kmeans.cluster_centers_.tolist(),
            silhouette_score=sil_score
        )
    
    except Exception as e:
        logger.error(f"Clustering error: {e}")
        raise HTTPException(status_code=500, detail=f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")

# LLM ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ë§
@app.post("/api/generate-labels")
async def generate_cluster_labels(request: LabelRequest):
    """LLMì„ í™œìš©í•œ í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒì„±"""
    try:
        if not openai_client:
            # OpenAI ì—†ì„ ë•Œ ê¸°ë³¸ ë¼ë²¨ë§
            return {"label": f"ê·¸ë£¹ ({len(request.words)}ê°œ ë‹¨ì–´)", "method": "fallback"}
        
        words_str = ", ".join(request.words)
        
        prompt = f"""
ë‹¤ìŒ í•œêµ­ì–´ ë‹¨ì–´ë“¤ì˜ ê³µí†µ ì£¼ì œë‚˜ ì¹´í…Œê³ ë¦¬ë¥¼ í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬ë¬¸ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ë‹¨ì–´ë“¤: {words_str}

ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ì˜ˆì‹œ:
- ë™ë¬¼, ìŠ¤í¬ì¸ , ìŒì‹, ê°€ì¡±, ê°ì •, ìƒ‰ê¹”, ìì—°, êµí†µ, ì „ìê¸°ê¸°, í•™êµ ë“±

ì ì ˆí•œ ì´ëª¨ì§€ì™€ í•¨ê»˜ ë‹µí•´ì£¼ì„¸ìš” (ì˜ˆ: ğŸ¾ ë™ë¬¼, âš½ ìŠ¤í¬ì¸ ):
"""

        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=20,
            temperature=0.3
        )
        
        label = response.choices[0].message.content.strip()
        
        return {
            "words": request.words,
            "label": label,
            "method": "gpt-3.5-turbo"
        }
    
    except Exception as e:
        logger.error(f"Label generation error: {e}")
        # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë¼ë²¨
        return {
            "words": request.words,
            "label": f"ğŸ“Š ê·¸ë£¹ {len(request.words)}",
            "method": "fallback",
            "error": str(e)
        }

# í—¬ìŠ¤ì²´í¬
@app.get("/api/health")
async def health_check():
    """API ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "openai_available": openai_client is not None,
        "cached_words": len(word_vectors),
        "cached_similarities": len(word_cache),
        "queue_size": len(word_queue)
    }

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv('PORT', 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)